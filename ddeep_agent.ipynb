{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5adb7edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f0a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab118946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "def internet_agent(\n",
    "        query: str,\n",
    "        max_results: int = 5,\n",
    "        topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "        include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"Run a web search\"\"\"\n",
    "    return tavily_client.search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic = topic\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3b62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ee0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt to steer the agent to be an expert researcher\n",
    "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
    "\n",
    "You have access to an internet agent tool as your primary means of gathering information.\n",
    "\n",
    "## `internet_agent`\n",
    "\n",
    "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
    "\"\"\"\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=[internet_agent],\n",
    "    system_prompt=research_instructions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d80c7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangGraph** is an open‑source framework created by the LangChain team that lets developers design, run, and manage *stateful, graph‑based AI agent workflows* in Python.  \n",
      "It builds on top of LangChain’s building blocks (LLMs, prompts, tools, memory, etc.) but adds a declarative graph model that makes it easier to:\n",
      "\n",
      "| Feature | What it means | Why it matters |\n",
      "|---------|---------------|----------------|\n",
      "| **Graph‑based architecture** | Agents are defined as nodes connected by edges, each edge representing a transition based on the agent’s output. | Visualizes the flow, makes debugging and iteration faster, and supports complex branching logic. |\n",
      "| **Stateful execution** | Each node can read/write a shared state dictionary that persists across turns. | Enables long‑running conversations, multi‑step reasoning, and memory of past interactions. |\n",
      "| **Built‑in memory** | Short‑term working memory + long‑term persistent memory (e.g., vector stores, databases). | Keeps context across sessions and allows agents to “remember” user preferences or prior tasks. |\n",
      "| **Tool integration** | Nodes can call external tools (Python functions, APIs, LLM calls) via LangChain’s tool interface. | Keeps the agent logic separate from the tool implementation, making it reusable. |\n",
      "| **Deployment & scaling** | Supports local execution, LangSmith integration, and a dedicated “LangGraph Platform” for production deployments. | Gives a production‑ready path from prototype to scalable service. |\n",
      "| **Debugging & observability** | LangSmith Studio visualizes the graph, shows node execution, state changes, and logs. | Makes it trivial to trace why an agent made a particular decision. |\n",
      "\n",
      "---\n",
      "\n",
      "### How it works (high‑level)\n",
      "\n",
      "1. **Define nodes** – Each node is a Python function or a LangChain chain that receives the current state and returns a new state or an action.  \n",
      "2. **Connect nodes with edges** – Edges can be conditional (based on the state) or unconditional.  \n",
      "3. **Create a `MessageGraph`** – The graph object orchestrates the flow, keeps track of the current node, and handles state persistence.  \n",
      "4. **Run the graph** – Call `graph.invoke(state)` or use the `AgentServer` API to expose the graph as a REST/WS endpoint.  \n",
      "5. **Observe** – Use LangSmith Studio to see the traversal, inspect state, and replay runs.\n",
      "\n",
      "---\n",
      "\n",
      "### Typical use cases\n",
      "\n",
      "| Use case | Why LangGraph is a good fit |\n",
      "|----------|-----------------------------|\n",
      "| **Conversational agents** | Stateful memory + branching logic for multi‑turn dialogue. |\n",
      "| **Task automation** | Break a complex task into sub‑tasks (e.g., booking a flight → search → book → confirm). |\n",
      "| **Multi‑agent systems** | Each agent is a sub‑graph; they can communicate via shared state or message passing. |\n",
      "| **ReAct agents** | The ReAct pattern (reason → act) maps naturally to a graph of reasoning and action nodes. |\n",
      "| **Explainable AI** | The graph structure provides a clear audit trail of decisions. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick example (pseudo‑code)\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, END\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# 1. Define a simple reasoning node\n",
      "def reason(state):\n",
      "    prompt = ChatPromptTemplate.from_template(\n",
      "        \"You are a helpful assistant. Given the user query: {question}\\n\"\n",
      "        \"First, think about the best next step.\"\n",
      "    )\n",
      "    llm = ChatOpenAI()\n",
      "    answer = llm.invoke({\"question\": state[\"question\"]})\n",
      "    return {\"thought\": answer.content}\n",
      "\n",
      "# 2. Define an action node\n",
      "def act(state):\n",
      "    # pretend we call an external API\n",
      "    result = f\"Performed action based on: {state['thought']}\"\n",
      "    return {\"response\": result, \"next\": END}\n",
      "\n",
      "# 3. Build the graph\n",
      "graph = StateGraph()\n",
      "graph.add_node(\"reason\", reason)\n",
      "graph.add_node(\"act\", act)\n",
      "graph.set_entry_point(\"reason\")\n",
      "graph.add_edge(\"reason\", \"act\")\n",
      "graph.add_edge(\"act\", END)\n",
      "\n",
      "# 4. Run\n",
      "state = {\"question\": \"Book a flight to Paris\"}\n",
      "final_state = graph.invoke(state)\n",
      "print(final_state[\"response\"])\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Where to learn more\n",
      "\n",
      "| Resource | What you’ll find |\n",
      "|----------|------------------|\n",
      "| **Official docs** – https://langchain-ai.github.io/langgraph/ | Full tutorials, API reference, and deployment guides. |\n",
      "| **GitHub repo** – https://github.com/langchain-ai/langgraph | Source code, examples, and issue tracker. |\n",
      "| **LangSmith Studio** – https://docs.langchain.com/langsmith/studio | Visual debugging and analytics for running graphs. |\n",
      "| **Tutorials** – e.g., “Build a ReAct agent with LangGraph” on Towards AI or the LangChain blog. | Step‑by‑step walkthroughs. |\n",
      "\n",
      "---\n",
      "\n",
      "### Bottom line\n",
      "\n",
      "LangGraph is a **graph‑oriented, stateful agent framework** that extends LangChain’s capabilities, making it easier to design complex, multi‑step AI workflows, debug them visually, and deploy them at scale. It’s especially useful when you need:\n",
      "\n",
      "* Clear, maintainable flow control  \n",
      "* Persistent memory across turns  \n",
      "* Integration with external tools or APIs  \n",
      "* Production‑ready observability and scaling\n",
      "\n",
      "If you’re already using LangChain for LLM prompts and tools, adding LangGraph lets you lift your agent logic into a reusable, observable graph that can grow from a simple chatbot to a full‑blown multi‑agent system.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
    "\n",
    "# Print the agent's response\n",
    "print(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
